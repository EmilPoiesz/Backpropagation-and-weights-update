{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {i : lambda x : (1 / (torch.cosh(x)**2)) for i in range(1, self.L+1)}\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # for output layer\n",
    "        delta = -2 * (y_true - y_pred) * model.df[model.L](model.z[model.L])\n",
    "        model.dL_dw[model.L] = torch.matmul(model.a[model.L-1].T, delta).T\n",
    "        model.dL_db[model.L] = delta[0]\n",
    "\n",
    "        # for each hidden layer\n",
    "        for l in reversed(range(1, model.L)):\n",
    "            ## this line has a mistake\n",
    "            delta = torch.matmul(delta, model.fc[str(l)].weight.data.T) * model.df[l](model.z[l])\n",
    "            ##\n",
    "            model.dL_dw[l] = torch.matmul(model.a[l-1].T, delta).T\n",
    "            model.dL_db[l] = delta[0]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.1472, 0.3628, 0.5374, 0.0308, 0.0048, 0.0148, 0.0006, 0.0008, 0.0, 0.0001]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-3.3784e-06, -2.3130e-06],\n",
      "        [-2.2277e-06, -1.5252e-06],\n",
      "        [-1.5052e-05, -1.0305e-05]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[1.9579e-05, 1.3404e-05],\n",
      "        [4.6638e-05, 3.1930e-05],\n",
      "        [4.3224e-05, 2.9593e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-3.2762e-07, -2.1603e-07, -1.4596e-06])\n",
      "  Autograd's computation:\n",
      " tensor([1.8986e-06, 4.5226e-06, 4.1915e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   1.1940\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   1.1940\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 2 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.7151, 0.4851, 0.2493, 0.0264, 0.0034, 0.0107, 0.0002, 0.0003, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-3.3174e-07, -2.2712e-07],\n",
      "        [-1.7342e-08, -1.1873e-08],\n",
      "        [-1.2867e-06, -8.8091e-07]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[2.7874e-06, 1.9084e-06],\n",
      "        [2.4896e-06, 1.7044e-06],\n",
      "        [4.8808e-06, 3.3416e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-3.2170e-08, -1.6817e-09, -1.2477e-07])\n",
      "  Autograd's computation:\n",
      " tensor([2.7030e-07, 2.4142e-07, 4.7330e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   1.1960\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   1.1960\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 3 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.5942, 0.5439, 0.2575, 0.0259, 0.0032, 0.0103, 0.0002, 0.0003, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-1.5242e-07, -1.0435e-07],\n",
      "        [-8.9961e-09, -6.1590e-09],\n",
      "        [-6.9582e-07, -4.7638e-07]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[1.3311e-06, 9.1134e-07],\n",
      "        [1.1213e-06, 7.6768e-07],\n",
      "        [2.7604e-06, 1.8899e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-1.4781e-08, -8.7237e-10, -6.7475e-08])\n",
      "  Autograd's computation:\n",
      " tensor([1.2908e-07, 1.0874e-07, 2.6768e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   1.2035\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   1.2035\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 4 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.4391, 0.5745, 0.2652, 0.0264, 0.0033, 0.0105, 0.0002, 0.0003, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-9.7533e-08, -6.6775e-08],\n",
      "        [-1.0298e-08, -7.0503e-09],\n",
      "        [-5.5401e-07, -3.7930e-07]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[8.7091e-07, 5.9626e-07],\n",
      "        [8.6500e-07, 5.9222e-07],\n",
      "        [2.2427e-06, 1.5354e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-9.4581e-09, -9.9861e-10, -5.3724e-08])\n",
      "  Autograd's computation:\n",
      " tensor([8.4455e-08, 8.3882e-08, 2.1748e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   1.2072\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   1.2072\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 5 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.5328, 0.5965, 0.2669, 0.0259, 0.0032, 0.0103, 0.0002, 0.0003, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-6.3528e-08, -4.3494e-08],\n",
      "        [-7.5370e-09, -5.1601e-09],\n",
      "        [-4.1729e-07, -2.8569e-07]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[5.8981e-07, 4.0381e-07],\n",
      "        [6.2733e-07, 4.2949e-07],\n",
      "        [1.7387e-06, 1.1904e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-6.1605e-09, -7.3088e-10, -4.0466e-08])\n",
      "  Autograd's computation:\n",
      " tensor([5.7196e-08, 6.0834e-08, 1.6861e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   1.2063\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   1.2063\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with autograd's computations.\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.4094, -0.5667],\n",
      "        [-0.4305, -0.4162],\n",
      "        [-0.2649, -0.6051]])\n",
      "tensor([ 0.3125, -0.5925, -0.2772])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 2 / 4: SOME TESTS FAILED, use 'verbose=True' and check the output for more details\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 576x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MyNet([\u001b[38;5;241m24\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackpropagation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/inf265/Project 1/project01/tests_backpropagation.py:271\u001b[0m, in \u001b[0;36mmain_test\u001b[0;34m(backprop_fn, model, eps, verbose, data)\u001b[0m\n\u001b[1;32m    269\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Training loop that compares our gradients with autograd's computations\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m autograd_ok, gradcheck_ok \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackprop_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m autograd_ok:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m TEST PASSED: Gradients consistent with autograd\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms computations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/inf265/Project 1/project01/tests_backpropagation.py:211\u001b[0m, in \u001b[0;36mcheck_gradients\u001b[0;34m(model, backprop_fn, optimizer, loss_fn, x, y_true, n_epochs, eps, verbose)\u001b[0m\n\u001b[1;32m    207\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, out_expected)\n\u001b[1;32m    209\u001b[0m loss\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 211\u001b[0m \u001b[43mbackprop_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_expected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m flag, res \u001b[38;5;241m=\u001b[39m grad_check( model, \u001b[38;5;28minput\u001b[39m, out_expected, loss_fn,eps\u001b[38;5;241m=\u001b[39meps )\n\u001b[1;32m    213\u001b[0m gradcheck_ok \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m flag\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mbackpropagation\u001b[0;34m(model, y_true, y_pred)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# for each hidden layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mL)):\n\u001b[0;32m---> 11\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mdf[l](model\u001b[38;5;241m.\u001b[39mz[l])\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mdL_dw[l] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(model\u001b[38;5;241m.\u001b[39ma[l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT, delta)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mdL_db[l] \u001b[38;5;241m=\u001b[39m delta[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 576x16)"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=True, data='mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
