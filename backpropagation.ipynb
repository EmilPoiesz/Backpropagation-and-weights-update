{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {i : lambda x : (1 / (torch.cosh(x)**2)) for i in range(1, self.L+1)}\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        delta = -2 * (y_true - y_pred)\n",
    "\n",
    "        for l in reversed(range(1, model.L+1)):\n",
    "\n",
    "            d_f = model.df[l](model.z[l])\n",
    "            delta = delta * d_f\n",
    "            \n",
    "            model.dL_dw[l] = torch.matmul(delta.T, model.a[l-1])\n",
    "            model.dL_db[l] = delta[0]\n",
    "\n",
    "            delta = torch.matmul(delta, model.fc[str(l)].weight.data)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 1.7922e-11,  1.8548e-11],\n",
      "        [-6.0872e-05, -6.2999e-05],\n",
      "        [ 1.0689e-09,  1.1062e-09]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 1.7922e-11,  1.8548e-11],\n",
      "        [-6.0872e-05, -6.2999e-05],\n",
      "        [ 1.0689e-09,  1.1062e-09]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 2.1033e-12, -7.1437e-06,  1.2544e-10])\n",
      "  Autograd's computation:\n",
      " tensor([ 2.1032e-12, -7.1437e-06,  1.2544e-10])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 2 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 9.8452e-12,  1.0189e-11],\n",
      "        [-3.1301e-05, -3.2395e-05],\n",
      "        [ 5.7135e-10,  5.9131e-10]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 9.8453e-12,  1.0189e-11],\n",
      "        [-3.1301e-05, -3.2395e-05],\n",
      "        [ 5.7135e-10,  5.9131e-10]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 1.1554e-12, -3.6734e-06,  6.7052e-11])\n",
      "  Autograd's computation:\n",
      " tensor([ 1.1554e-12, -3.6734e-06,  6.7052e-11])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 3 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 6.9956e-12,  7.2400e-12],\n",
      "        [-2.1286e-05, -2.2030e-05],\n",
      "        [ 3.9955e-10,  4.1351e-10]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 6.9956e-12,  7.2400e-12],\n",
      "        [-2.1286e-05, -2.2030e-05],\n",
      "        [ 3.9955e-10,  4.1351e-10]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 8.2098e-13, -2.4980e-06,  4.6890e-11])\n",
      "  Autograd's computation:\n",
      " tensor([ 8.2097e-13, -2.4980e-06,  4.6890e-11])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 4 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 5.5007e-12,  5.6929e-12],\n",
      "        [-1.6178e-05, -1.6743e-05],\n",
      "        [ 3.1062e-10,  3.2147e-10]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 5.5008e-12,  5.6929e-12],\n",
      "        [-1.6178e-05, -1.6743e-05],\n",
      "        [ 3.1062e-10,  3.2147e-10]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 6.4555e-13, -1.8986e-06,  3.6453e-11])\n",
      "  Autograd's computation:\n",
      " tensor([ 6.4555e-13, -1.8986e-06,  3.6453e-11])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 5 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 4.5643e-12,  4.7238e-12],\n",
      "        [-1.3070e-05, -1.3527e-05],\n",
      "        [ 2.5557e-10,  2.6450e-10]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 4.5643e-12,  4.7238e-12],\n",
      "        [-1.3070e-05, -1.3527e-05],\n",
      "        [ 2.5557e-10,  2.6450e-10]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 5.3565e-13, -1.5339e-06,  2.9992e-11])\n",
      "  Autograd's computation:\n",
      " tensor([ 5.3565e-13, -1.5339e-06,  2.9992e-11])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.8933, -0.6927],\n",
      "        [ 0.1294,  0.6069],\n",
      "        [-0.6721, -0.6987]])\n",
      "tensor([ 0.0737, -0.2627,  0.1988])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emil/opt/anaconda3/envs/inf265/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[3.3559e-115, 3.3559e-115, 3.3559e-115,  ..., 3.3559e-115,\n",
      "         3.3559e-115, 3.3559e-115],\n",
      "        [-1.7124e-34, -1.7124e-34, -1.7124e-34,  ..., -1.7124e-34,\n",
      "         -1.7124e-34, -1.7124e-34],\n",
      "        [-1.0897e-99, -1.0897e-99, -1.0897e-99,  ..., -1.0897e-99,\n",
      "         -1.0897e-99, -1.0897e-99],\n",
      "        ...,\n",
      "        [2.9892e-104, 2.9892e-104, 2.9892e-104,  ..., 2.9892e-104,\n",
      "         2.9892e-104, 2.9892e-104],\n",
      "        [-6.3804e-61, -6.3804e-61, -6.3804e-61,  ..., -6.3804e-61,\n",
      "         -6.3804e-61, -6.3804e-61],\n",
      "        [-4.8096e-10, -4.8096e-10, -4.8096e-10,  ..., -4.8096e-10,\n",
      "         -4.8096e-10, -4.8096e-10]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-4.8096e-10, -4.8096e-10, -4.8096e-10,  ..., -4.8096e-10,\n",
      "         -4.8096e-10, -4.8096e-10]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-7.9144e-115,   4.0385e-34,   2.5698e-99,  -3.8939e-02,   5.2355e-06,\n",
      "          6.6082e-28,   2.8102e-73,  -4.7112e-32,   5.4351e-58,   3.8350e-09,\n",
      "         -8.7396e-99,   9.6240e-61,   2.0440e-91, -7.0496e-104,   1.5047e-60,\n",
      "          1.1343e-09])\n",
      "  Autograd's computation:\n",
      " tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.8939e-02,  5.2355e-06,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.8350e-09,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.1343e-09])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.4379, -0.3931, -0.4165,  ..., -0.3669, -0.3818, -0.4006],\n",
      "        [ 0.1177,  0.0987,  0.0822,  ...,  0.1123,  0.1207,  0.1009],\n",
      "        [ 0.3366,  0.3265,  0.3848,  ...,  0.3600,  0.3571,  0.3134],\n",
      "        ...,\n",
      "        [-0.3315, -0.3865, -0.3347,  ..., -0.3972, -0.3750, -0.3311],\n",
      "        [ 0.2053,  0.1794,  0.2323,  ...,  0.2383,  0.1716,  0.2316],\n",
      "        [ 0.0158,  0.0531,  0.0738,  ...,  0.0273,  0.0398,  0.0339]])\n",
      "tensor([ 0.9350, -0.2673, -0.8508, -0.0119, -0.1728, -0.2287, -0.6093, -0.2886,\n",
      "         0.4685,  0.0378, -0.8109, -0.5340,  0.7264,  0.8451, -0.4594, -0.0914])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=True, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f3155c26dc513e518498c54b5f9ff85ad01473166d581020fa91a3170cc06ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
